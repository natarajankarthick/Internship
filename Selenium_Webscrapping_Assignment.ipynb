{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af9371-fde6-4123-bf58-f9e54c55a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required. \n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs \n",
    "The task will be done as shown in the below steps: \n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field. \n",
    "3. Then click the search button. \n",
    "4. Then apply the location filter and salary filter by checking the respective boxes \n",
    "5. Then scrape the data for the first 10 jobs results you get. \n",
    "6. Finally create a dataframe of the scraped data\n",
    "'''\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path\n",
    "\n",
    "#Selenium WebDriver with the correct path to chromedriver\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to your chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.get(\"https://www.naukri.com/\") \n",
    "\n",
    "\n",
    "time.sleep(25)  # Wait for the page to fully load\n",
    "#print(driver.page_source)  \n",
    "\n",
    "#  \"Data Scientist\" in the search field \n",
    "search_box = driver.find_element(By.CLASS_NAME, \"suggestor-input\")\n",
    "search_box.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Submit the search\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "time.sleep(5)  # Allow search results to load\n",
    "\n",
    "# location filter (Delhi/NCR)\n",
    "location_filter = driver.find_element(By.XPATH, \"//span[text()='Delhi / NCR']\")\n",
    "location_filter.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# salary filter (3-6 Lakhs)\n",
    "salary_filter = driver.find_element(By.XPATH, \"//span[text()='3-6 Lakhs']\")\n",
    "salary_filter.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Scrape the job data for the first 10 results\n",
    "jobs_data = []\n",
    "for i in range(1, 11):\n",
    "    try:\n",
    "        # Extract job title\n",
    "        job_title = driver.find_element(By.XPATH, f\"(//a[@class='title fw500 ellipsis'])[{i}]\").text\n",
    "        \n",
    "        # Extract company name\n",
    "        company_name = driver.find_element(By.XPATH, f\"(//a[@class='subTitle ellipsis fleft'])[{i}]\").text\n",
    "        \n",
    "        # Extract job location\n",
    "        job_location = driver.find_element(By.XPATH, f\"(//li[@class='fleft grey-text br2 placeHolderLi location'])[{i}]\").text\n",
    "        \n",
    "        # Extract experience required\n",
    "        experience_required = driver.find_element(By.XPATH, f\"(//li[@class='fleft grey-text br2 placeHolderLi experience'])[{i}]\").text\n",
    "        \n",
    "        # Append scraped data\n",
    "        jobs_data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Company Name': company_name,\n",
    "            'Job Location': job_location,\n",
    "            'Experience Required': experience_required\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping job {i}: {e}\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(jobs_data)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "df.to_csv('naukri_job_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e54d3-b749-43f6-b9e3-228f8c1a67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the \n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data. \n",
    "'''\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # path to Chrome executable\n",
    "\n",
    "# Set Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "#driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Webpage\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "\n",
    "# Wait  page to load\n",
    "time.sleep(5)  \n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in \"Job title, Skills\" field and \"Bangalore\" in \"enter the location\" field\n",
    "job_title_field = WebDriverWait(driver, 70).until(EC.presence_of_element_located((By.XPATH, \"//input[@id='id_q']\")))  \n",
    "job_title_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "location_field = driver.find_element(By.XPATH, \"//input[@id='id_loc']\")  # location field\n",
    "location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@class='searchBtn']\")  # Search button click\n",
    "search_button.click()\n",
    "\n",
    "# Wait for search results to load\n",
    "time.sleep(5)  # Adjust sleep time if needed\n",
    "\n",
    "# Scrape the data for the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "jobs = WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='jobTuple']\")))[:10]\n",
    "\n",
    "for job in jobs:\n",
    "    title = job.find_element(By.XPATH, \".//h3[@class='job_title']/a\").text\n",
    "    location = job.find_element(By.XPATH, \".//ul[@class='job_desc']/li[@class='loc']\").text\n",
    "    company = job.find_element(By.XPATH, \".//ul[@class='job_desc']/li[@class='comp']\").text\n",
    "    experience = job.find_element(By.XPATH, \".//ul[@class='job_desc']/li[@class='exp']\").text\n",
    "    \n",
    "    job_titles.append(title)\n",
    "    job_locations.append(location)\n",
    "    company_names.append(company)\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# CreateDataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "})\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96959c0-4328-48c1-97c1-a3e06c0a4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuote\n",
    "3. Than scrap a)Quote b) Author c) Type Of Quote\n",
    "\n",
    "'''\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# Set up the Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  #  path to Chrome executable\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to  executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Get the webpage\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "# Click on \"Top Quotes\" \n",
    "top_quotes_link = WebDriverWait(driver, 40).until(\n",
    "    EC.presence_of_element_located((By.XPATH, \"//a[contains(text(), 'Top Quotes')]\"))\n",
    ")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Wait for the Top Quotes page to load\n",
    "time.sleep(5)  \n",
    "\n",
    "# Scrape the data for Top 1000 Quotes\n",
    "quotes_list = []\n",
    "authors_list = []\n",
    "categories_list = []\n",
    "\n",
    "# Loop through pages and extract quotes, authors, and categories\n",
    "for page in range(1, 21):  #50 PER PAGE\n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Scrape the quotes, authors, and types\n",
    "    quotes = driver.find_elements(By.CLASS_NAME, 'quote')\n",
    "    for quote in quotes:\n",
    "        quote_text = quote.find_element(By.CLASS_NAME, 'title').text\n",
    "        author_text = quote.find_element(By.CLASS_NAME, 'author').text\n",
    "        type_of_quote = quote.find_element(By.CLASS_NAME, 'tags').text\n",
    "\n",
    "        quotes_list.append(quote_text)\n",
    "        authors_list.append(author_text)\n",
    "        categories_list.append(type_of_quote)\n",
    "\n",
    "   # Click the next button to go to the next page using XPATH\n",
    "\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//li[@class='next']/a\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "    except Exception as e:\n",
    "        print(f\"No more pages or issue finding the next button: {e}\")\n",
    "        break\n",
    "\n",
    "\n",
    "# Create a DataFrame \n",
    "df = pd.DataFrame({\n",
    "    'Quote': quotes_list,\n",
    "    'Author': authors_list,\n",
    "    'Type of Quote': categories_list\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('top_1000_quotes.csv', index=False)\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d449c69-e58d-4f9f-9223-020415c0e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” inthe search\n",
    "field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price'''\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # path toChrome executable\n",
    "\n",
    "# Set Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.set_page_load_timeout(60) \n",
    "\n",
    "# Open Flipkart\n",
    "driver.get(\"https://www.flipkart.com\")\n",
    "\n",
    "# Close login pop-up\n",
    "try:\n",
    "    close_button = driver.find_element(By.XPATH, \"//button[contains(text(), '✕')]\")\n",
    "    close_button.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Search for sneakers\n",
    "search_box = driver.find_element(By.NAME, \"q\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(3)  \n",
    "\n",
    "# Scraping first 100 \n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "while len(brands) < 100:\n",
    "    # Find sneaker details \n",
    "    sneaker_elements = driver.find_elements(By.XPATH, \"//div[contains(@class, '_2WkVRV')]\")  # Brand\n",
    "    description_elements = driver.find_elements(By.XPATH, \"//a[contains(@class, 'IRpwTa')]\")  # Product Description\n",
    "    price_elements = driver.find_elements(By.XPATH, \"//div[contains(@class, '_30jeq3')]\")  # Price\n",
    "\n",
    "    for i in range(len(sneaker_elements)):\n",
    "        if len(brands) >= 100:\n",
    "            break\n",
    "        try:\n",
    "            brands.append(sneaker_elements[i].text)\n",
    "            descriptions.append(description_elements[i].text)\n",
    "            prices.append(price_elements[i].text)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Scroll down \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  \n",
    "\n",
    "# Closing the driver\n",
    "driver.quit()\n",
    "\n",
    "# Save the data to a CSV file\n",
    "sneaker_data = pd.DataFrame({\n",
    "    'Brand': brands,\n",
    "    'Product Description': descriptions,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "sneaker_data.to_csv('sneakers_data.csv', index=False)\n",
    "print(\"Data saved to sneakers_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42a5ce7-330b-4ac6-8063-8bd0b70c0a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to laptops_data.csv\n",
      "                                               Title Ratings   Price\n",
      "0                                                N/A             N/A\n",
      "1                                                N/A     N/A     N/A\n",
      "2  Acer Aspire Lite 13th Gen Intel Core i3-1305U ...          33,990\n",
      "3  Acer Aspire Lite AMD Ryzen 5 5500U Premium Thi...          34,990\n",
      "4  Lenovo V15 G2 15.6\" FHD Anti-Glare Business La...     N/A  22,990\n",
      "5  Acer Aspire 3 Laptop Intel Core Celeron N4500 ...          21,990\n",
      "6  Lenovo IdeaPad Slim 1 Intel Core Celeron N4020...          25,990\n",
      "7                                                N/A          33,990\n",
      "8                                                N/A          33,990\n",
      "9                                                N/A          53,990\n"
     ]
    }
   ],
   "source": [
    "'''Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "Type filter to “Intel Core i7” as shown in the below image:\n",
    "\n",
    "Aftersetting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price '''\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # path to Chrome executable\n",
    "\n",
    "# Set Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.set_page_load_timeout(60) \n",
    "\n",
    "# Open Amazon India\n",
    "driver.get(\"https://www.amazon.in\")\n",
    "\n",
    "# Search for 'Laptop'\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"Laptop\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(3) \n",
    "\n",
    "# CPU filter to \"Intel Core i7\"\n",
    "try:\n",
    "    filter_element = driver.find_element(By.XPATH, \"//li[@aria-label='Intel Core i7']\")\n",
    "    filter_element.click()\n",
    "    time.sleep(3) \n",
    "except:\n",
    "    print(\"Intel Core i7 filter not found or couldn't be applied\")\n",
    "\n",
    "# Scrape data first 10 laptops\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "laptops = driver.find_elements(By.XPATH, \"//div[contains(@class,'s-result-item')]\")[:10]  # Get first 10 laptops\n",
    "\n",
    "for laptop in laptops:\n",
    "    try:\n",
    "        # Title\n",
    "        title = laptop.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\").text\n",
    "        titles.append(title)\n",
    "    except:\n",
    "        titles.append(\"N/A\")\n",
    "\n",
    "    try:\n",
    "        # Ratings\n",
    "        rating = laptop.find_element(By.XPATH, \".//span[@class='a-icon-alt']\").text\n",
    "        ratings.append(rating)\n",
    "    except:\n",
    "        ratings.append(\"N/A\")\n",
    "\n",
    "    try:\n",
    "        # Price\n",
    "        price = laptop.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        prices.append(price)\n",
    "    except:\n",
    "        prices.append(\"N/A\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Store data in a DataFrame and export to CSV\n",
    "laptop_data = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Ratings': ratings,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "laptop_data.to_csv('laptops_data.csv', index=False)\n",
    "print(\"Data saved to laptops_data.csv\")\n",
    "\n",
    "# Display DataFrame\n",
    "print(laptop_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b6c36-7dd2-4bb6-b8fe-f4338e942a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=F\n",
    "LIPKAR.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # path to Chrome executable\n",
    "\n",
    "# Setthe Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.set_page_load_timeout(60) \n",
    "\n",
    "# Flipkart reviews page for iPhone 11\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "# Scroll down to more reviews\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(3)  # Allow reviews to load after scrolling\n",
    "\n",
    "# Scroll again more reviews \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll a few times to load more reviews \n",
    "for _ in range(5):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Give time for more reviews to load\n",
    "\n",
    "# Scrape data\n",
    "reviews_collected = 0\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "while reviews_collected < 100:\n",
    "    # Find all review containers\n",
    "    reviews = driver.find_elements(By.XPATH, \"//div[@class='_1AtVbE']\")\n",
    "\n",
    "    for review in reviews:\n",
    "        if reviews_collected >= 100:\n",
    "            break\n",
    "\n",
    "        # Scrape the rating\n",
    "        try:\n",
    "            rating = review.find_element(By.XPATH, \".//div[@class='_3LWZlK _1BLPMq']\").text\n",
    "        except:\n",
    "            rating = \"N/A\"\n",
    "\n",
    "        # Scrape the review summary\n",
    "        try:\n",
    "            review_summary = review.find_element(By.XPATH, \".//p[@class='_2-N8zT']\").text\n",
    "        except:\n",
    "            review_summary = \"N/A\"\n",
    "\n",
    "        # Scrape the full review\n",
    "        try:\n",
    "            full_review = review.find_element(By.XPATH, \".//div[@class='t-ZTKy']\").text\n",
    "        except:\n",
    "            full_review = \"N/A\"\n",
    "\n",
    "        # Append the data\n",
    "        ratings.append(rating)\n",
    "        review_summaries.append(review_summary)\n",
    "        full_reviews.append(full_review)\n",
    "\n",
    "        reviews_collected += 1\n",
    "\n",
    "    #Click on the next button to load more reviews\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@class='_1LKTO3'][contains(text(), 'Next')]\")\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # Allow the next page to load\n",
    "    except:\n",
    "        print(\"No more pages or failed to click Next.\")\n",
    "        break \n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save the data to a DataFrame and CSV\n",
    "reviews_data = pd.DataFrame({\n",
    "    'Rating': ratings,\n",
    "    'Review Summary': review_summaries,\n",
    "    'Full Review': full_reviews\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "reviews_data.to_csv('iphone11_reviews.csv', index=False)\n",
    "print(\"Data saved to iphone11_reviews.csv\")\n",
    "\n",
    "# Display DataFrame\n",
    "print(reviews_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62be17e-34d9-4dda-87d5-86fab7270328",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to display list of respected former Prime Ministers of India (i.e. Name,\n",
    "Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of\u0002all-prime-ministers-of-india-1473165149-1\n",
    "scrap the mentioned data and make the DataFramE'''\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # path Chrome executable\n",
    "\n",
    "# Setthe Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.set_page_load_timeout(60) \n",
    "\n",
    "# target webpage\n",
    "driver.get(\"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\")\n",
    "time.sleep(5)  # Wait for the page to fully load\n",
    "\n",
    "# section containing Prime Ministers' list\n",
    "table_section = driver.find_element(By.XPATH, \"//div[@class='table-box']\")\n",
    "rows = table_section.find_elements(By.TAG_NAME, \"tr\")[1:]  \n",
    "\n",
    "# empty list to store the data\n",
    "prime_ministers_data = []\n",
    "\n",
    "# Loop through each row of the table\n",
    "for row in rows:\n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    \n",
    "    if len(cols) == 4:\n",
    "        name = cols[0].text\n",
    "        born_dead = cols[1].text\n",
    "        term_of_office = cols[2].text\n",
    "        remarks = cols[3].text\n",
    "        \n",
    "        # Append the data to the list\n",
    "        prime_ministers_data.append({\n",
    "            \"Name\": name,\n",
    "            \"Born-Dead\": born_dead,\n",
    "            \"Term of Office\": term_of_office,\n",
    "            \"Remarks\": remarks\n",
    "        })\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame using Pandas\n",
    "df = pd.DataFrame(prime_ministers_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"List of Indian PMs\", dataframe=df)\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df.to_csv(\"prime_ministers_of_india.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c833f5d-bada-4646-98d1-ad1fdd161971",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a python program to display list of 50 Most expensive cars in the world\n",
    "(i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap thementioned data and make the dataframe '''\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # path Chrome executable\n",
    "\n",
    "# Setthe Selenium WebDriver\n",
    "\n",
    "path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path chromedriver executable\n",
    "service = Service(path_chromedriver)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.set_page_load_timeout(60)\n",
    "\n",
    "# Open Motor1 website\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "time.sleep(5)  # Allow the page to load\n",
    "\n",
    "# Locate the search bar \n",
    "search_bar = driver.find_element(By.CSS_SELECTOR, \"input[type='search']\")\n",
    "search_bar.send_keys(\"50 most expensive cars\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "time.sleep(5)  \n",
    "\n",
    "# Click on the link to the article for '50 most expensive cars in the world'\n",
    "article_link = driver.find_element(By.PARTIAL_LINK_TEXT, \"50 Most Expensive Cars In The World\")\n",
    "article_link.click()\n",
    "time.sleep(5)  \n",
    "\n",
    "# Scrape the car names and prices\n",
    "car_data = []\n",
    "\n",
    "# Look for the container that holds car names and prices\n",
    "car_items = driver.find_elements(By.XPATH, \"//div[@class='listicle__item']\")\n",
    "\n",
    "for car_item in car_items:\n",
    "    try:\n",
    "        # Extract car name\n",
    "        car_name = car_item.find_element(By.XPATH, \".//h2\").text\n",
    "        \n",
    "        # Extract price\n",
    "        car_price = car_item.find_element(By.XPATH, \".//p[contains(text(),'$')]\").text\n",
    "        \n",
    "        # Append data to the list\n",
    "        car_data.append({\n",
    "            'Car Name': car_name,\n",
    "            'Price': car_price\n",
    "        })\n",
    "    except:\n",
    "        continue  # Skip any items that don’t match the structure\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(car_data)\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Most Expensive Cars\", dataframe=df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv(\"most_expensive_cars.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b2b0a-def0-4913-939e-5f0a7ef6121e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f401a103-48d9-4ff6-a556-23db2fa8a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found.\n"
     ]
    }
   ],
   "source": [
    "#write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "#heading, date, content and the likes for the video from the link for the youtube video from the post\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def getdata(url):\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    return response.text\n",
    "\n",
    "def parse_post_data(html_content):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # List to store post details\n",
    "    postsdat = []\n",
    "    \n",
    "   \n",
    "    posts = soup.find_all('div', {'class': 'sc-1f2ug0x-0'})  # Replace with correct class or tag\n",
    "    \n",
    "    for post in posts:\n",
    "       \n",
    "        heading = post.find('h2').get_text(strip=True)\n",
    "        \n",
    "       \n",
    "        date = post.find('time').get_text(strip=True)\n",
    "        \n",
    "       \n",
    "        content = post.find('div', {'class': 'sc-1d6vudc-1'}).get_text(strip=True)  \n",
    "        \n",
    "        \n",
    "        likes = post.find('span', {'class': 'sc-1dxpml1-0'}).get_text(strip=True)  # Replace with correct class\n",
    "        \n",
    "        # Extracting YouTube video link (assuming it's in an anchor tag within the post content)\n",
    "        youtubelink = None\n",
    "        for link in post.find_all('a', href=True):\n",
    "            if 'youtube.com' in link['href']:\n",
    "                youtubelink = link['href']\n",
    "                break\n",
    "        \n",
    "        postsdat.append({\n",
    "            'Heading': heading,\n",
    "            'Date': date,\n",
    "            'Content': content,\n",
    "            'Likes': likes,\n",
    "            'YouTube Link': youtubelink\n",
    "        })\n",
    "    \n",
    "    return postsdat\n",
    "\n",
    "def create_dataframe(posts_data):\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(posts_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    weburl = 'https://www.patreon.com/coreyms'\n",
    "    html_content = getdata(weburl)\n",
    "    posts_data = parse_post_data(html_content)\n",
    "    \n",
    "    if posts_data:  # Ensure there is data to display\n",
    "        df = create_dataframe(posts_data)\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de92822f-9855-42b3-8fe3-4d72afaf1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No house data found.\n"
     ]
    }
   ],
   "source": [
    "'''Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page_content(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    return response.text\n",
    "\n",
    "def parse_house_data(html_content):\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "   \n",
    "    houses_data = []\n",
    "    \n",
    "    # Find all house sections (assuming each house is within a certain HTML structure)\n",
    "    houses = soup.find_all('div', {'class': 'card'})  \n",
    "    \n",
    "    for house in houses:\n",
    "      \n",
    "        title = house.find('span', {'class': 'house-title'}).get_text(strip=True)  \n",
    "        \n",
    "       \n",
    "        location = house.find('div', {'class': 'location'}).get_text(strip=True)  \n",
    "        \n",
    "        \n",
    "        area = house.find('div', {'class': 'area'}).get_text(strip=True)  \n",
    "        \n",
    "        \n",
    "        emi = house.find('div', {'class': 'emi'}).get_text(strip=True)  \n",
    "        \n",
    "       \n",
    "        price = house.find('div', {'class': 'price'}).get_text(strip=True)  \n",
    "        # Append \n",
    "        houses_data.append({\n",
    "            'Title': title,\n",
    "            'Location': location,\n",
    "            'Area': area,\n",
    "            'EMI': emi,\n",
    "            'Price': price\n",
    "        })\n",
    "    \n",
    "    return houses_data\n",
    "\n",
    "def create_dataframe(houses_data):\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(houses_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Define the URL template and the localities\n",
    "    base_url = 'https://www.nobroker.in/property/sale/{locality}?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciJ9XQ==&radius=2.0&city=bangalore&locality=Indiranagar'\n",
    "    #https://www.nobroker.in/property/sale/bangalore/Indiranagar?searchParam=W3sibGF0IjoxMi45NzgzNjkyLCJsb24iOjc3LjY0MDgzNTYsInBsYWNlSWQiOiJDaElKa1FOM0dLUVdyanNSTmhCUUpyaEdEN1UiLCJwbGFjZU5hbWUiOiJJbmRpcmFuYWdhciJ9XQ==&radius=2.0&city=bangalore&locality=Indiranagar\n",
    "    \n",
    "    # List to store all houses data across the localities\n",
    "    all_houses_data = []\n",
    "    \n",
    "    localities = ['bangalore/Indiranagar', 'bangalore/Jayanagar', 'bangalore/Rajajinagar']\n",
    "    \n",
    "    for locality in localities:\n",
    "        url = base_url.replace('{locality}', locality)\n",
    "        html_content = fetch_page_content(url)\n",
    "        houses_data = parse_house_data(html_content)\n",
    "        all_houses_data.extend(houses_data)\n",
    "    \n",
    "    if all_houses_data:  # Ensure there is data to display\n",
    "        df = create_dataframe(all_houses_data)\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No house data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "646e9c03-3b70-422f-a9d1-0c6b3e21ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Product Name Price  \\\n",
      "0  bewakoof x tom & jerry   N/A   \n",
      "1       bewakoof x marvel   N/A   \n",
      "2               Bewakoof®   N/A   \n",
      "3               Bewakoof®   N/A   \n",
      "4           bewakoof x dc   N/A   \n",
      "5           bewakoof x dc   N/A   \n",
      "6               Bewakoof®   N/A   \n",
      "7               Bewakoof®   N/A   \n",
      "8         bewakoof x nasa   N/A   \n",
      "9      bewakoof x peanuts   N/A   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://images.bewakoof.com/t640/women-s-blue-...  \n",
      "1  https://images.bewakoof.com/t640/men-s-black-m...  \n",
      "2  https://images.bewakoof.com/t640/women-s-red-i...  \n",
      "3  https://images.bewakoof.com/t640/women-s-red-e...  \n",
      "4  https://images.bewakoof.com/t640/bat-call-velc...  \n",
      "5  https://images.bewakoof.com/t640/men-s-black-a...  \n",
      "6  https://images.bewakoof.com/t640/women-s-black...  \n",
      "7  https://images.bewakoof.com/t640/women-s-orang...  \n",
      "8  https://images.bewakoof.com/t640/men-s-red-nas...  \n",
      "9  https://images.bewakoof.com/t640/men-s-black-n...  \n"
     ]
    }
   ],
   "source": [
    "'''Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "https://www.bewakoof.com/bestseller?sort=popular '''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    return response.text\n",
    "\n",
    "def parse_product_data(html_content):\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # List \n",
    "    products_data = []\n",
    "    \n",
    "    \n",
    "    products = soup.find_all('div', {'class': 'productCardBox'}, limit=12)  # Limit to 12\n",
    "    \n",
    "    for product in products:\n",
    "        # Extracting the product name\n",
    "        name_tag = product.find('h3')\n",
    "        name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
    "        \n",
    "        \n",
    "        price_tag = product.find('span', {'class': 'discountedPriceText'})\n",
    "        price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
    "        \n",
    "        \n",
    "        image_tag = product.find('img')\n",
    "        image_url = image_tag['src'] if image_tag else \"N/A\"\n",
    "        \n",
    "        # Append the extracted data to the ;ist\n",
    "        products_data.append({\n",
    "            'Product Name': name,\n",
    "            'Price': price,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "    \n",
    "    return products_data\n",
    "\n",
    "def create_dataframe(products_data):\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(products_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "    html_content = fetch_page_content(url)\n",
    "    products_data = parse_product_data(html_content)\n",
    "    \n",
    "    if products_data:  # Ensure there is data to display\n",
    "        df = create_dataframe(products_data)\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No product data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3219d05b-4c4d-4e7e-98c3-fe2398342b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Heading Date  \\\n",
      "0  Treasury yields slip ahead of consumer inflati...  N/A   \n",
      "1  Wednesday's big stock stories: What’s likely t...  N/A   \n",
      "2  Stock futures are little changed ahead of key ...  N/A   \n",
      "3  Stocks making the biggest moves after hours: A...  N/A   \n",
      "4  Buy these stocks with solid earnings power dur...  N/A   \n",
      "5  Analyst calls: Starbucks gets upgraded, a buyi...  N/A   \n",
      "6  CIO makes the case for UK stocks despite U.S. ...  N/A   \n",
      "7  Jefferies names its 3 favorite chip stocks — g...  N/A   \n",
      "8  These 4 Japanese stocks are trading at a 'deep...  N/A   \n",
      "9  'Get Britain building again': New UK finance c...  N/A   \n",
      "\n",
      "                                                Link  \n",
      "0  https://www.cnbc.com/2024/08/14/us-treasurys-a...  \n",
      "1  https://www.cnbc.com/2024/08/13/wednesdays-big...  \n",
      "2  https://www.cnbc.com/2024/08/13/stock-market-t...  \n",
      "3  https://www.cnbc.com/2024/08/13/stocks-making-...  \n",
      "4  https://www.cnbc.com/2024/08/14/these-stocks-a...  \n",
      "5  https://www.cnbc.com/2024/08/14/analyst-calls-...  \n",
      "6  https://www.cnbc.com/2024/08/14/cio-makes-the-...  \n",
      "7  https://www.cnbc.com/2024/08/14/jefferies-name...  \n",
      "8  https://www.cnbc.com/2024/08/14/4-japanese-sto...  \n",
      "9  https://www.cnbc.com/2024/07/08/uk-election-20...  \n"
     ]
    }
   ],
   "source": [
    "'''Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    " a) headings\n",
    "b) date\n",
    "c) News linK '''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    return response.text\n",
    "\n",
    "def parse_news_data(html_content):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # List \n",
    "    news_data = []\n",
    "    \n",
    "    \n",
    "    articles = soup.find_all('div', {'class': 'Card-standardBreakerCard'}, limit=12)\n",
    "    \n",
    "    for article in articles:\n",
    "        # Extracting heading\n",
    "        heading_tag = article.find('a', {'class': 'Card-title'})\n",
    "        heading = heading_tag.get_text(strip=True) if heading_tag else \"N/A\"\n",
    "        \n",
    "        \n",
    "        date_tag = article.find('time')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else \"N/A\"\n",
    "        \n",
    "        \n",
    "        link = heading_tag['href'] if heading_tag and heading_tag.has_attr('href') else \"N/A\"\n",
    "        \n",
    "        # Append to list\n",
    "        news_data.append({\n",
    "            'Heading': heading,\n",
    "            'Date': date,\n",
    "            'Link': link\n",
    "        })\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "def create_dataframe(news_data):\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(news_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.cnbc.com/world/?region=world'\n",
    "    html_content = fetch_page_content(url)\n",
    "    news_data = parse_news_data(html_content)\n",
    "    \n",
    "    if news_data:  # Ensure there is data to display\n",
    "        df = create_dataframe(news_data)\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No news data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f95b807-591b-4bc4-a4c4-af2db84d41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " 6) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded\u0002articles/ and scrap\u0002a) Paper title\n",
    " b) date\n",
    "c) Author\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    return response.text\n",
    "\n",
    "def parse_data(html_content):\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # List \n",
    "    papers_data = []\n",
    "    \n",
    "    # Find all \n",
    "    papers = soup.find_all('div', {'class': 'article-item'})\n",
    "    \n",
    "    for paper in papers:\n",
    "        \n",
    "        title_tag = paper.find('h2')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "        \n",
    "        \n",
    "        date_tag = paper.find('span', {'class': 'meta-item'})\n",
    "        date = date_tag.get_text(strip=True) if date_tag else \"N/A\"\n",
    "        \n",
    "        \n",
    "        author_tag = paper.find('p', {'class': 'authors'})\n",
    "        author = author_tag.get_text(strip=True) if author_tag else \"N/A\"\n",
    "        \n",
    "        # Append data to list\n",
    "        papers_data.append({\n",
    "            'Title': title,\n",
    "            'Date': date,\n",
    "            'Author': author\n",
    "        })\n",
    "    \n",
    "    return papers_data\n",
    "\n",
    "def create_dataframe(papers_data):\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(papers_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "    html_content = fetch_page_content(url)\n",
    "    pdata = parse_data(html_content)\n",
    "    \n",
    "    if pdata:  # Ensure there is data to display\n",
    "        df = create_dataframe(pdata)\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20aacf5b-0410-4fef-ba6c-8c2e86d71720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Year of Release]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame.\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL \n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "movies = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "# Initialize lists \n",
    "movie_names = []\n",
    "movie_ratings = []\n",
    "movie_years = []\n",
    "\n",
    "\n",
    "for movie in movies:\n",
    "    # Movie name\n",
    "    name = movie.h3.a.text\n",
    "    movie_names.append(name)\n",
    "    \n",
    "    # Movie year\n",
    "    year = movie.h3.find('span', class_='lister-item-year').text.strip('() ')\n",
    "    movie_years.append(year)\n",
    "    \n",
    "    # Movie rating\n",
    "    rating = movie.find('span', class_='ipl-rating-star__rating').text if movie.find('span', class_='ipl-rating-star__rating') else 'N/A'\n",
    "    movie_ratings.append(rating)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame({\n",
    "    'Name': movie_names,\n",
    "    'Rating': movie_ratings,\n",
    "    'Year of Release': movie_years\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da74bb4-97df-4550-aa09-4016fc9c6d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

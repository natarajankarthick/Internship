{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c53b57-486b-4c6b-bc9c-5591b93a7324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Rank  \\\n",
      "0                               \"Baby Shark Dance\"[7]   \n",
      "1                                     \"Despacito\"[10]   \n",
      "2                                 \"See You Again\"[21]   \n",
      "3                                \"Gangnam Style\"⁂[31]   \n",
      "4                                         \"Baby\"*[69]   \n",
      "5                                   \"Bad Romance\"[73]   \n",
      "6                         \"Charlie Bit My Finger\"[77]   \n",
      "7   \"Evolution of Dance\" (3rd time as most viewed ...   \n",
      "8                               \"Girlfriend\"‡[81][82]   \n",
      "9   \"Evolution of Dance\" (2nd time as most viewed ...   \n",
      "10                     \"Music Is My Hot Hot Sex\"‡[87]   \n",
      "11                          \"Evolution of Dance\"*[79]   \n",
      "12                   \"Pokémon Theme Music Video\"‡[92]   \n",
      "13                     \"Myspace – The Movie\"‡[97][98]   \n",
      "14                          \"Phony Photo Booth\"‡[101]   \n",
      "15                  \"The Chronic of Narnia Rap\"‡[107]   \n",
      "16                 \"Ronaldinho: Touch of Gold\"‡*[110]   \n",
      "17                                 \"I/O Brush\"‡*[116]   \n",
      "\n",
      "                                           Name         Artist  \\\n",
      "0   Pinkfong Baby Shark - Kids' Songs & Stories  7,046,700,000   \n",
      "1                                    Luis Fonsi  2,993,700,000   \n",
      "2                                   Wiz Khalifa  2,894,000,000   \n",
      "3                                           Psy    803,700,000   \n",
      "4                                 Justin Bieber    245,400,000   \n",
      "5                                     Lady Gaga    178,400,000   \n",
      "6                                         HDCYT    128,900,000   \n",
      "7                                Judson Laipply    118,900,000   \n",
      "8                                   RCA Records     92,600,000   \n",
      "9                                Judson Laipply     78,400,000   \n",
      "10                               CLARUSBARTEL72     76,600,000   \n",
      "11                               Judson Laipply     10,600,000   \n",
      "12                                        Smosh      4,300,000   \n",
      "13                                       eggtea      2,700,000   \n",
      "14                                    mugenized      3,400,000   \n",
      "15                                  youtubedude      2,300,000   \n",
      "16                                   Nikesoccer        255,000   \n",
      "17                                       larfus        247,000   \n",
      "\n",
      "          Upload Date              Views  \n",
      "0       June 17, 2016   November 2, 2020  \n",
      "1    January 12, 2017     August 4, 2017  \n",
      "2       April 6, 2015      July 10, 2017  \n",
      "3       July 15, 2012  November 24, 2012  \n",
      "4   February 19, 2010      July 16, 2010  \n",
      "5   November 24, 2009     April 14, 2010  \n",
      "6        May 22, 2007   October 25, 2009  \n",
      "7       April 6, 2006        May 2, 2009  \n",
      "8   February 27, 2007      July 17, 2008  \n",
      "9       April 6, 2006     March 15, 2008  \n",
      "10      April 9, 2007      March 1, 2008  \n",
      "11      April 6, 2006       May 19, 2006  \n",
      "12  November 28, 2005     March 12, 2006  \n",
      "13   January 31, 2006  February 18, 2006  \n",
      "14   December 1, 2005   January 21, 2006  \n",
      "15  December 18, 2005    January 9, 2006  \n",
      "16   October 21, 2005   October 31, 2005  \n",
      "17    October 5, 2005   October 29, 2005  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Qn1:\n",
    "Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date \n",
    "E) View\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Exception handling block\n",
    "try:\n",
    "    # GET request to Wikipedia page\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Raise an exception for failures\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    \n",
    "    if table is None:\n",
    "        raise Exception(\"Could not find the table on the Wikipedia page.\")\n",
    "\n",
    "    # Scrape the details (Rank, Name, Artist, Upload date, Views)\n",
    "    videos = []\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        # Check if row has expected number of columns\n",
    "        if len(cols) < 5:\n",
    "            continue  # Skip rows that don't have enough columns\n",
    "\n",
    "        try:\n",
    "            \n",
    "            rank = cols[0].text.strip()\n",
    "\n",
    "            # Name \n",
    "            name = cols[1].text.strip().replace('\"', '')\n",
    "\n",
    "            # Artist/Creator\n",
    "            artist = cols[2].text.strip()\n",
    "\n",
    "            # Upload date\n",
    "            upload_date = cols[3].text.strip()\n",
    "\n",
    "            # View count\n",
    "            views = cols[4].text.strip()\n",
    "\n",
    "            \n",
    "            videos.append({\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Artist': artist,\n",
    "                'Upload Date': upload_date,\n",
    "                'Views': views\n",
    "            })\n",
    "        \n",
    "        except IndexError as e:\n",
    "            print(f\"Error while accessing table columns: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while scraping the row: {e}\")\n",
    "\n",
    "    # Convert the data to a DataFrame\n",
    "    df = pd.DataFrame(videos)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred while making the HTTP request: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848e461f-5886-44ba-927e-4764a65b7aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find the Fixtures button. Exiting.\n"
     ]
    }
   ],
   "source": [
    "'''Qn2:\n",
    "\n",
    "Scrape the details team India’s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code.\n",
    "\n",
    "'''\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "import time\n",
    "\n",
    "# scrape the data\n",
    "def scrape_bcci_fixtures():\n",
    "    try:\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path\n",
    "\n",
    "        #Selenium WebDriver with path to chromedriver\n",
    "        path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  \n",
    "        service = Service(path_chromedriver)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        driver.get('https://www.bcci.tv/')\n",
    "\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Navigate to international fixtures page\n",
    "        try:\n",
    "            fixtures_button = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//a[@href='/international/fixtures']\"))\n",
    "            )\n",
    "            fixtures_button.click()\n",
    "        except TimeoutException:\n",
    "            print(\"Couldn't find the Fixtures button.\")\n",
    "            driver.quit()\n",
    "            return\n",
    "        \n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'fixture-card')))\n",
    "\n",
    "        # Extract fixtures\n",
    "        fixtures = driver.find_elements(By.CLASS_NAME, 'fixture-card')\n",
    "\n",
    "        for fixture in fixtures:\n",
    "            try:\n",
    "                # Extract series \n",
    "                series_name = fixture.find_element(By.CLASS_NAME, 'fixture-card__series-name').text\n",
    "                \n",
    "                # Extract location\n",
    "                place = fixture.find_element(By.CLASS_NAME, 'fixture-card__venue').text\n",
    "                \n",
    "                # Extract date,time\n",
    "                date_time = fixture.find_element(By.CLASS_NAME, 'fixture-card__datetime').text\n",
    "                \n",
    "                # Split date,time\n",
    "                date, time_ = date_time.split(\"•\")\n",
    "\n",
    "                # Print details\n",
    "                print(f\"Series: {series_name}\\nPlace: {place}\\nDate: {date.strip()}\\nTime: {time_.strip()}\\n\")\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(\"An element was not found on the fixture card. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"Error with the WebDriver: {str(e)}\")\n",
    "    finally:\n",
    "        # Close the driver\n",
    "        driver.quit()\n",
    "\n",
    "# Call scraping function\n",
    "scrape_bcci_fixtures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e81d6c-57ab-476b-bdfd-f37b5c04f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economy page link not found or page took too long to load.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Qn3:\n",
    "\n",
    "Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details: A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n",
    "\n",
    "'''\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape state-wise GDP details\n",
    "def scrape_gdp_data():\n",
    "    try:\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path\n",
    "\n",
    "        \n",
    "        path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "        service = Service(path_chromedriver)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)    \n",
    "        \n",
    "        # Navigate to homepage\n",
    "        driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "        # Wait for page to load and click on the Economy link\n",
    "        try:\n",
    "          \n",
    "            economy_link = WebDriverWait(driver, 50).until(\n",
    "                EC.presence_of_element_located((By.LINK_TEXT, \"Economy\"))\n",
    "            )\n",
    "            economy_link.click()\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Economy page link not found or page took too long to load.\")\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "        # Wait for the State-wise GDP section to load\n",
    "        try:\n",
    "            gdp_link = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.LINK_TEXT, \"Indian states by GDP\"))\n",
    "            )\n",
    "            gdp_link.click()\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"GDP page link not found or page took too long to load.\")\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "        # scrape the data.\n",
    "        try:\n",
    "           \n",
    "            table = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"display.dataTable\"))\n",
    "            )\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "            # Extract header \n",
    "            headers = [header.text for header in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "\n",
    "            # Extract data rowwise\n",
    "            data = []\n",
    "            for row in rows[1:]:\n",
    "                cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if len(cols) == 6:  # Ensure there are 6 columns as per the required data\n",
    "                    data.append({\n",
    "                        'Rank': cols[0].text,\n",
    "                        'State': cols[1].text,\n",
    "                        'GSDP(18-19)': cols[2].text,\n",
    "                        'GSDP(19-20)': cols[3].text,\n",
    "                        'Share(18-19)': cols[4].text,\n",
    "                        'GDP($ billion)': cols[5].text\n",
    "                    })\n",
    "\n",
    "            # Store the data DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            print(df)\n",
    "            # Save to a CSV file\n",
    "            df.to_csv('india_gdp_by_state.csv', index=False)\n",
    "\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Unable to locate the GDP data table.\", e)\n",
    "        \n",
    "    except WebDriverException as e:\n",
    "        print(\"Error initializing WebDriver\", e)\n",
    "    \n",
    "    finally:\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "# Call scraper function\n",
    "scrape_gdp_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53185d9e-1deb-417e-96f2-467a21d6151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Qn4:\n",
    "Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used \n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code.\n",
    "'''\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "import pandas as pd\n",
    "\n",
    "# Exception handling block\n",
    "try:\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path    \n",
    "    path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "    service = Service(path_chromedriver)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)  \n",
    "\n",
    "    # Step 1: Open the GitHub homepage\n",
    "    driver.get(\"https://github.com/\")\n",
    "    \n",
    "    # Step 2: Wait until the 'Explore' link is available and click on it\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    explore_menu = wait.until(EC.element_to_be_clickable((By.XPATH, \"//summary[contains(text(),'Explore')]\")))\n",
    "    explore_menu.click()\n",
    "    \n",
    "    # Step 3: Wait until the 'Trending' link is available and click on it\n",
    "    trending_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(),'Trending')]\")))\n",
    "    trending_link.click()\n",
    "\n",
    "    # Step 4: Scrape details for the trending repositories\n",
    "    trending_repos = []\n",
    "    repo_elements = driver.find_elements(By.XPATH, \"//article[@class='Box-row']\")\n",
    "\n",
    "    for repo in repo_elements:\n",
    "        try:\n",
    "            # Repository Title\n",
    "            repo_title = repo.find_element(By.XPATH, \".//h1/a\").text\n",
    "            \n",
    "            # Repository Description\n",
    "            try:\n",
    "                repo_description = repo.find_element(By.XPATH, \".//p\").text\n",
    "            except NoSuchElementException:\n",
    "                repo_description = \"No description provided\"\n",
    "            \n",
    "            # Contributors Count\n",
    "            try:\n",
    "                contributors = repo.find_elements(By.XPATH, \".//a[contains(@href, '/graphs/contributors')]\")\n",
    "                contributors_count = len(contributors)\n",
    "            except NoSuchElementException:\n",
    "                contributors_count = \"Not available\"\n",
    "            \n",
    "            # Language Used\n",
    "            try:\n",
    "                language = repo.find_element(By.XPATH, \".//span[@itemprop='programmingLanguage']\").text\n",
    "            except NoSuchElementException:\n",
    "                language = \"Not specified\"\n",
    "\n",
    "            # Append data to list\n",
    "            trending_repos.append({\n",
    "                \"Repository Title\": repo_title,\n",
    "                \"Repository Description\": repo_description,\n",
    "                \"Contributors Count\": contributors_count,\n",
    "                \"Language Used\": language\n",
    "            })\n",
    "        \n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Could not find an element for a repository: {e}\")\n",
    "\n",
    "    # Step 5: Convert to DataFrame and display\n",
    "    df = pd.DataFrame(trending_repos)\n",
    "    print(df)\n",
    "    \n",
    "except NoSuchElementException as e:\n",
    "    print(\"Element not found on the page:\", e)\n",
    "except TimeoutException as e:\n",
    "    print(\"Loading took too much time:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e90fa0-6875-4ad5-8d1e-94e801dc65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Qn5:\n",
    "Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board \n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code. \n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Exception handling block\n",
    "try:\n",
    "    chrome_options = Options()\n",
    "    chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path    \n",
    "    path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "    service = Service(path_chromedriver)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "    # Step 1: Open the Billboard website homepage\n",
    "    driver.get(\"https://www.billboard.com/\")\n",
    "\n",
    "    # Step 2: Wait until the 'Charts' link is available and click on it\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    charts_link = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, \"Charts\")))\n",
    "    charts_link.click()\n",
    "\n",
    "    # Step 3: Wait until the 'Hot 100' link is available and click on it\n",
    "    hot_100_link = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, \"Hot 100\")))\n",
    "    hot_100_link.click()\n",
    "\n",
    "    # Step 4: Scrape details for the top 100 songs\n",
    "    songs_data = []\n",
    "    song_elements = driver.find_elements(By.XPATH, \"//ul[@class='o-chart-results-list-row']\")\n",
    "\n",
    "    for song in song_elements[:100]:  # Extract details for the top 100\n",
    "        try:\n",
    "            # Song Name\n",
    "            song_name = song.find_element(By.XPATH, \".//h3\").text\n",
    "\n",
    "            # Artist Name\n",
    "            artist_name = song.find_element(By.XPATH, \".//span[@class='c-label']\").text\n",
    "\n",
    "            # Last Week Rank\n",
    "            last_week_rank = song.find_element(By.XPATH, \".//li[@class='lrv-u-width-100p u-width-full']/span[1]\").text\n",
    "\n",
    "            # Peak Rank\n",
    "            peak_rank = song.find_element(By.XPATH, \".//li[@class='lrv-u-width-100p u-width-full']/span[2]\").text\n",
    "\n",
    "            # Weeks on Board\n",
    "            weeks_on_board = song.find_element(By.XPATH, \".//li[@class='lrv-u-width-100p u-width-full']/span[3]\").text\n",
    "\n",
    "            # Append data to list\n",
    "            songs_data.append({\n",
    "                \"Song Name\": song_name,\n",
    "                \"Artist Name\": artist_name,\n",
    "                \"Last Week Rank\": last_week_rank,\n",
    "                \"Peak Rank\": peak_rank,\n",
    "                \"Weeks on Board\": weeks_on_board\n",
    "            })\n",
    "        \n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Could not find an element for a song: {e}\")\n",
    "\n",
    "    # Step 5: Convert to DataFrame and display\n",
    "    df = pd.DataFrame(songs_data)\n",
    "    print(df)\n",
    "    \n",
    "except NoSuchElementException as e:\n",
    "    print(\"Element not found on the page:\", e)\n",
    "except TimeoutException as e:\n",
    "    print(\"Loading took too much time:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8c1f93-6525-4561-9bb4-f5b653b64b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table found, extracting data...\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Qn6: Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre \n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    " \n",
    "''' \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape highest-selling novel details\n",
    "def scrape_best_selling_books():\n",
    "    try:\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path\n",
    "\n",
    "        \n",
    "        path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "        service = Service(path_chromedriver)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Navigate to the  URL\n",
    "        driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "\n",
    "        # Wait for the page to load and locate table containing book data\n",
    "        try:\n",
    "            # locate the table using XPath or CSS selector\n",
    "            table = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//table\"))\n",
    "            )\n",
    "            print(\"Table found, extracting data...\")\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Table not found or page took too long to load.\")\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "        # Scrape table rows\n",
    "        try:\n",
    "            # Find all rows in table\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "            # Extract headers \n",
    "            headers = [header.text for header in rows[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "\n",
    "            # Extract data from each row \n",
    "            data = []\n",
    "            for row in rows[1:]:\n",
    "                cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if len(cols) == 5:  # Ensure there are 5 columns as per the required data\n",
    "                    data.append({\n",
    "                        'Book Name': cols[0].text,\n",
    "                        'Author Name': cols[1].text,\n",
    "                        'Volumes Sold': cols[2].text,\n",
    "                        'Publisher': cols[3].text,\n",
    "                        'Genre': cols[4].text\n",
    "                    })\n",
    "\n",
    "            # Store data in a pandas DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            print(df)\n",
    "\n",
    "            # Save to a CSV file\n",
    "            df.to_csv('best_selling_books.csv', index=False)\n",
    "\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Unable to locate table rows or data.\", e)\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(\"Error initializing WebDriver\", e)\n",
    "\n",
    "    finally:\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "# Call function\n",
    "scrape_best_selling_books()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d588dd-e23a-477e-9576-1cce35e30b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV series list not found or page took too long to load.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Qn 7:\n",
    "\n",
    "Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/ You have \n",
    "to find the following details: \n",
    "A) Name \n",
    "B) Year span \n",
    "C) Genre \n",
    "D) Run time \n",
    "E) Ratings \n",
    "F) Votes\n",
    "\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape most-watched TV series from IMDb\n",
    "def scrape_most_watched_tv_series():\n",
    "    try:\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path\n",
    "\n",
    "        #Selenium WebDriver with the correct path to chromedriver\n",
    "        path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to your chromedriver executable\n",
    "        service = Service(path_chromedriver)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        # Navigate to the IMDb list URL\n",
    "        driver.get(\"https://www.imdb.com/list/ls053826112/\")\n",
    "\n",
    "        # Wait for the page to load and locate the TV series elements\n",
    "        try:\n",
    "            # Wait until the list of TV series is present on the page\n",
    "            tv_series_elements = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='lister-item-content']\"))\n",
    "            )\n",
    "            print(\"TV series elements found, extracting data...\")\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"TV series list not found or page took too long to load.\")\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "        # Scrape data from the TV series elements\n",
    "        try:\n",
    "            data = []\n",
    "            for series in tv_series_elements:\n",
    "                try:\n",
    "                    # Extracting the required fields\n",
    "                    name = series.find_element(By.XPATH, \".//h3/a\").text\n",
    "                    year_span = series.find_element(By.XPATH, \".//h3/span[contains(@class, 'lister-item-year')]\").text\n",
    "                    genre = series.find_element(By.XPATH, \".//span[@class='genre']\").text.strip()\n",
    "                    run_time = series.find_element(By.XPATH, \".//span[@class='runtime']\").text\n",
    "                    rating = series.find_element(By.XPATH, \".//div[@class='ipl-rating-star small']//span[@class='ipl-rating-star__rating']\").text\n",
    "                    votes = series.find_element(By.XPATH, \".//span[@name='nv']\").text\n",
    "\n",
    "                    # Append the data into a dictionary\n",
    "                    data.append({\n",
    "                        'Name': name,\n",
    "                        'Year Span': year_span,\n",
    "                        'Genre': genre,\n",
    "                        'Run Time': run_time,\n",
    "                        'Ratings': rating,\n",
    "                        'Votes': votes\n",
    "                    })\n",
    "\n",
    "                except NoSuchElementException as e:\n",
    "                    print(f\"Error extracting data for a TV series: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Store the data in a pandas DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            print(df)\n",
    "\n",
    "            # Save the data to a CSV file\n",
    "            df.to_csv('most_watched_tv_series.csv', index=False)\n",
    "\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Unable to locate necessary elements.\", e)\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(\"Error initializing WebDriver\", e)\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "# Run the scraper function\n",
    "scrape_most_watched_tv_series()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88e6edb-3778-4af8-a847-ed2a3bc1c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading took too much time!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Qn8:\n",
    "Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details: \n",
    "A) Dataset name \n",
    "B) Data type \n",
    "C) Task \n",
    "D) Attribute type \n",
    "E) No of instances \n",
    "F) No of attribute G) Year \n",
    " Note: - from the home page you have to go to the Show All Dataset page through code.\n",
    " '''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_details(url):\n",
    "  \n",
    "\n",
    "  try:\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"  # Path\n",
    "\n",
    "      \n",
    "    path_chromedriver = r'C:\\chromedriver-win64\\chromedriver.exe'  # path to chromedriver executable\n",
    "    service = Service(path_chromedriver)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the \"View All Datasets\" button to be clickable and then click it\n",
    "    view_all_datasets_button = WebDriverWait(driver, 30).until(\n",
    "        EC.element_to_be_clickable((By.LINK_TEXT, \"View All Datasets\"))\n",
    "    )\n",
    "    view_all_datasets_button.click()\n",
    "\n",
    "    # Wait for the dataset table to load\n",
    "    dataset_table = WebDriverWait(driver, 30).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table.table\"))\n",
    "    )\n",
    "\n",
    "    # Extract dataset details from the table\n",
    "    rows = dataset_table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip the header row\n",
    "    dataset_details = []\n",
    "    for row in rows:\n",
    "      columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "      dataset_name = columns[0].text.strip()\n",
    "      data_type = columns[1].text.strip()\n",
    "      task = columns[2].text.strip()\n",
    "      attribute_type = columns[3].text.strip()\n",
    "      no_of_instances = columns[4].text.strip()\n",
    "      no_of_attribute = columns[5].text.strip()\n",
    "      year = columns[6].text.strip()\n",
    "\n",
    "      dataset_details.append({\n",
    "          \"Dataset Name\": dataset_name,\n",
    "          \"Data Type\": data_type,\n",
    "          \"Task\": task,\n",
    "          \"Attribute Type\": attribute_type,\n",
    "          \"No of Instances\": no_of_instances,\n",
    "          \"No of Attributes\": no_of_attribute,\n",
    "          \"Year\": year\n",
    "      })\n",
    "\n",
    "    return dataset_details\n",
    "\n",
    "  except TimeoutException:\n",
    "    print(\"Loading took too much time!\")\n",
    "  except NoSuchElementException:\n",
    "    print(\"Could not find the element!\")\n",
    "  finally:\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  url = \"https://archive.ics.uci.edu/\"\n",
    "  datasets = get_dataset_details(url)\n",
    "  if datasets:\n",
    "    for dataset in datasets:\n",
    "      print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc084caa-4223-49b4-a5a1-e1c9b598b7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
